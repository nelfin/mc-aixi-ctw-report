\documentclass[pdftex,twoside,a4paper]{report}
\usepackage[pdftex]{graphicx}
\usepackage[margin=3.0cm]{geometry}
\usepackage[english]{babel}
\usepackage[normalem]{ulem}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage[sc]{mathpazo}
\usepackage[round]{natbib}

\newcommand{\hs}{$\hspace{0.5cm}$}
\newcommand{\bt}{\begin{tabbing}}
\newcommand{\et}{\end{tabbing}}
\newcommand{\bcen}{\begin{center}}
\newcommand{\ecen}{\end{center}}
\newcommand{\mac}{MC-AIXI-CTW}

\begin{document}

\begin{titlepage}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\begin{center}

\textsc{\Large Research School of Computer Science}\\[0.5cm]
\textsc{\Large College of Engineering and}\\[0.2cm]
\textsc{\Large Computer Science}\\[0.5cm]
\vspace{1.4cm}
\hrule
\vspace{1.4cm}
{\huge \bfseries An Implementation of MC-AIXI-CTW} \\
\vspace{0.4cm}


\begin{tabular}{ccccc}
  Jarrah Bloomfield\footnotemark &
  Luke English\footnotemark &
  Andrew Haigh\footnotemark &
  Joshua Nelson\footnotemark &
  Anthony Voutas\footnotemark
\end{tabular}

\vspace{1.4cm}
\hrule
\vspace{1.0cm}
\textsc{\large COMP4620 - Advanced Artificial Intelligence}\\
\textsc{Assignment 2}\\
\vspace{1.0cm}
\hrule
\vspace{1.4cm}
\vfill
%Bottom of the page
{\large \today} \\[0.5cm]

\begin{tabular}{ccccc}
  \setcounter{footnote}{0}
  u4838292\footnotemark &
  u4667844\footnotemark &
  u4667010\footnotemark &
  u4850020\footnotemark &
  u4519169\footnotemark
\end{tabular}
\end{center}
 
\end{titlepage}
\chapter{Description of the MC-AIXI-CTW implementation}

The AIXI agent is a formal, mathematical agent which represents a solution to
the general reinforcement learning problem. Principally, AIXI consists of an
expectimax search over a Bayesian mixture of Turing machines in order to
choose optimal actions by predicting future observations and rewards based on
past experience. Given infinite computational resources, AIXI represents the
optimal reinforcement agent: maximising future expected reward in any unknown
environment.

In the far more limited world of what is tractable, we require an
approximation to AIXI. Here, we approximate AIXI via a combination of UCT
search (Monte-Carlo Tree Search with the Upper Confidence Bound)
\citep{kocsis2006bandit} and Context Tree Weighting
\citep{willems1995context}, yielding MC-AIXI-CTW.

\chapter{User Manual}
\section{Arguments}
The agent can be compile with the \texttt{make} command. The agent then can then be run using\\

\texttt{./main <environment> <logfile>}\\

\texttt{<environment>} is a compulsory argument, which specifies the environment configuration file the agent is to use. In this implementation, it is one of the following
\begin{itemize}
\item \texttt{coinflip.conf}: Biased coin flip enviroment
\item \texttt{grid.conf}: Gridworld environment
\item \texttt{kuhnpoker.conf}: Kuhn poker environment
\item \texttt{pacman.conf}: Pacman environment
\item \texttt{rps.conf}: Biased rock paper scissors environment
\item \texttt{tiger.conf}: ``Tiger'' Environment
\item \texttt{composite.conf}: A combination of the above environments
\end{itemize}
\texttt{<logfile>} is an optional argument, which specifies the name of a log file to output results to.
\newline
\section{Configuration files}
\texttt{.conf} files are \emph{configuration files}, specifying which environment is to be used, and relevant parameters for each environment. Each configuration file has the following parameters
\begin{itemize}
\item \texttt{environment}: The name of the environment to use. One of \{4x4-grid, kuhn-poker, tiger, biased-rock-paper-scissor, pacman, composite\}.
\item \texttt{exploration}: The rate at which the agent explores, by making random decisions.
\item \texttt{explore-decay }: The rate at the exploration rate decreases
\end{itemize}
In addition to this, some configurations have parameters that are specific to their environments.
\begin{itemize}
\item Coinflip
    \begin{itemize}
        \item \texttt{coin-flip-p}: The probability of a flipping heads (0 $\leq$ \texttt{coin-flip-p}  $\leq$ 1).
    \end{itemize}
\item Kuhn poker
    \begin{itemize}
        \item \texttt{gamma}: A constant that determines the environment's Nash equilibrium strategy. (0 $\leq$ \texttt{gamma}  $\leq$ 1)
    \end{itemize}
\item Pacman
    \begin{itemize}
        \item \texttt{mapfile}: The location of the map file for the pacman board.
    \end{itemize}
\item tiger.conf
    \begin{itemize}
        \item \texttt{left-door-p}: The probability that the gold is behind the left door
        \item \texttt{listen-p}: The probability that a listening observation is correct.
    \end{itemize}
\item composite.conf
    \begin{itemize}
        \item \texttt{environmentN}: Specifies the $N^{\text{th}}$ environment, where $0 \leq N \leq 10$. The value of this parameter is an integer $\leq 10$, and indicates which environment environmentN represents.
        \item \texttt{startN}: Specifies the time step that at which the $N^{\text{th}}$ environment starts, where $0 \leq N \leq 10$.
        \item Paremeters required for the environemnts $1..N$ specified in \texttt{environment.cpp}.
    \end{itemize}
\end{itemize}

\chapter{MC-AIXI-CTW Implementation}
\section{Design Choices}

We have implemented MC-AIXI-CTW in the C++ coding language, which has a number of benefits. Firstly, C++ is a better option for implementation than C or another imperative language, as objects allow us to pass recursive and enumerated structures between processes. Secondly, C++ is a much more low-level language than any other object-oriented language, which allows us to perform much more optimal processing; and dynamic memory allocation means that we can ensure that our program doesn't take up more memory than we need it to, so that it will overall take less time to run.

\section{Context Tree Weighting}
The algorithm for implementing context-tree weighting (CTW) mainly consists of the methods and objects given in the file \texttt{predict.cpp}, along with its corresponding \texttt{.hpp} file. We run through these from the beginning of the file to its end.  

\subsection{Objects}
\begin{itemize}
\item{\texttt{CTNode}

This object represents a node in the context tree; and so it needs to store both the Laplacian estimate of the probability, along with the weight given to it by the actual CTW algorithm. For mathematical stability, we store these floating-point numbers as logarithms, so we can just add them instead of multiplying them. In addition, it has a 2-array of pointers to its left and right child, and it stores the counts for these nodes in the context of the history (which is also required for the CTW algorithm).
  }
\item{\texttt{ContextTree}

Here we have the entire context tree represented as an object, which is a glorified pointer to the root node of the tree (as \texttt{CTNode}s store their own children). In addition to storing the root, \texttt{ContextTree} also stores a double-ended queue of symbols (boolean integers) which represents the current history of the tree, and an unsigned integer which represents the maximum depth of the tree (so that we don't add nodes past the depth, and take up more memory than we want).
  }
\end{itemize}

\subsection{Methods}
\subsubsection{\texttt{CTNode}}
\begin{itemize}
\item{\texttt{logProbWeighted, logProbEstimated, visits, child}

    These methods are simply defined. Since the values stored in variables such as \texttt{m\_log\_prob\_weighted} are declared privately, it is required to use a public method to get them. \texttt{visits} grabs the sum of the counts for the child nodes - the number of times that we have seen this particular node in our travels; and \texttt{child} takes a symbol, and returns a pointer to the correct (left or right) child.
}
\item{\texttt{update}

    \texttt{update} is one of the most important methods in the entire CTW implementation. We recurse through the tree, starting from the node that calls this method, popping symbols from the end of the history until we reach a leaf; as in the CTW algorithm. When we do, we update the counts for the leaf based on the symbol that is passed to \texttt{update}, and then follow the branch back up the tree, using the equation given for $P_{w}$: \[ P_{w} = \frac{1}{2}P_{KT}(n) + \frac{1}{2}P^{0}P^{1}. \] % can someone check this against the literature?
This method doesn't change the history, as we push the popped symbols back onto the stack when we're done with each call.
  }
\item{\texttt{size}
    
    This is a simple method; it counts the total size of the subtree with the calling node as the root. 
  }
\item{\texttt{logKTMul}
    
    This method calculates the Laplacian estimator $P_{KT}$ for the node, given a symbol $n \in \{0,1\}$, where \[ P_{KT}(n) = \frac{\#n + 0.5}{\#n + \#(1-n) + 1}.\] This forms the base multiplier for the probabilities in the context tree.
  }
\item{\texttt{revert}
    
    If \texttt{update} is the method which adds a symbol to the context tree, then \texttt{revert} is the method which removes it. We again recurse down the tree, removing the symbol from the leaf, and pushing the changes back up the tree using the current history. Again, we don't change the history with this method.
  }
\item{\texttt{prettyPrintNode}
    
    Mainly used for testing, this method prints the counts and probabilities at a node, indented given the depth in the tree.
  }
\end{itemize}
\subsubsection{\texttt{ContextTree}}
\begin{itemize}
\item{\texttt{clear}
    
    Instead of using the generic destructor \texttt{~ContextTree}, this method simply deletes the existing history and disassociates the pointer to the root, making a new root and associating the tree with that root instead.
  }
\item{\texttt{update}

    This is an overloaded method, which can either take in a symbol, or a list of symbols. If passed a list, it simply runs itself on each of the symbols in turn; and if passed a symbol, it first checks whether we have enough pre-history to generate a tree. If we don't, it simply pushes the symbol to the back of the history (where we can easily access it again), and if we do, then we need to actually change nodes; so we call the \texttt{update} method from \texttt{CTNode} instead, before pushing the symbol onto the history.
  }
\item{\texttt{updateHistory}
    
    Here we simply push a list of symbols onto the back of the history.
  }
\item{\texttt{revert}
    
    This version of \texttt{revert} pops the last symbol off the history, and uses it to pass to the \texttt{CTNode} version. It is useful because only the \texttt{ContextTree} has access to the variable \texttt{m\_history}.
  }
\item{\texttt{revertHistory}
    
    Unlike \texttt{revert}, this method takes in a size to revert back to (this corresponds to a previous age of the context tree), and simply pops symbols from the history until we get back to that size. 
  }
\item{\texttt{genRandomSymbols}
    
    This method utilises the following \texttt{genRandomSymbolsAndUpdate} method to make a string of random symbols on the history, utilising \texttt{revert} in order to remove the changes that the following method makes to the tree.
  }
\item{\texttt{genRandomSymbolsAndUpdate}
    
    As the name implies, this method uses the context-tree weights to form a random distribution, upon which it generates \texttt{bits} bits and pushes them to the given symbol list, and then updates the context-tree weightings.
  }
\item{\texttt{logBlockProbability}
    
    This is a simple getter method for the weighted probability of the root - which is the probability of the entire context tree.
  }
\item{\texttt{nthHistorySymbol}
    
    Again, this method is aptly named: it returns the $n$th most recent history symbol. (Of course, when it can't find a symbol back that far, it just returns \texttt{NULL} instead.)
  }
\item{\texttt{depth, historySize, size}
    
    These are simple getter methods for the private variables contained within a \texttt{ContextTree}.
  }
\item{\texttt{predictNext}
    
    This method uses the weighted context-tree to make an educated guess about what the next symbol may be. It predicts the probability of a 1 being the next symbol, then uses a random generator to figure out whether it should guess 1 or 0.
  }
\item{\texttt{prettyPrint, printHistory}
    
    These methods are printing methods - \texttt{prettyPrint} uses the \texttt{prettyPrintNode} method from \texttt{CTNode} to recursively print the entire tree, indented by level; and \texttt{printHistory} just prints a single string of each symbol in the history, with spaces for separators.
  }
\end{itemize}
\section{Upper Confidence Tree}
Most of the Upper Confidence Tree (UCT) algorithm is implemented in the files \texttt{search.cpp}. The UCT algorithm assumes we have some model of the environment (CTW) and searches this model for the best action.
\subsection{Objects}
\begin{itemize}
	\item{\texttt{SearchNode}
	
	This object represents a node in the search tree. The \texttt{SearchNodes} form an expectimax tree, alternating between decision and chance nodes. \texttt{SearchNode} contains most of the methods related to traversing this tree, and calculating optimal actions from it.
	}
	
	\item{\texttt{ModelUndo}
	
	This object represents a snapshot of an agent's model of the environment at a given time. The agent includes functions to revert its model (the CTW tree) to the time at which the \texttt{ModelUndo} was created. This functionality is necessary in the \texttt{sample} function, as it makes changes to the CTW tree that need to be undone.
	
	}
\end{itemize}
\subsection{Methods}

\subsubsection*{\texttt{SearchNode}}
\begin{itemize}
	\item{\texttt{selectAction}
	
	Determines the next action to play, starting from this search node. \texttt{selectAction} looks through the possible actions it could take from this point, and chooses the action that would yield the best expected score (based on the sampling this node in the \texttt{sample} function). Included in this expected score is the \texttt{ucb\_bound} variable, which gives a bonus to nodes that have not been well explored.
	}
	
	\item{\texttt{sample}
	
	Performs a trial run through this node, and through it's children. \texttt{sample} calculates the expected reward along the way, which updates the expected score (\texttt{m\_mean}) value for \texttt{selectAction}. This function generates actions to take as it travels down the tree using the CTW tree (\texttt{agent::genPerceptAndUpdate} and \texttt{agent::modelUpdate}).
	
	If \texttt{SearchNode} is a chance node, it generates an observation and reward from the CTW tree, and continues down through \texttt{SearchNode}'s children. If it is a decision node, it uses \texttt{SelectAction} to choose an action. On the first visit, it uses the \texttt{playout} function to determine a reward.
	}
	
	\item{\texttt{playout}
	
	Generates actions randomly, and updates the CTW tree based on them. Used in the first run through of the \texttt{sample} function. 
	}
\end{itemize}

\subsubsection{\texttt{ModelUndo}}
\begin{itemize}
\item{\texttt{ModelUndo}

	The constructor for a \texttt{ModelUndo} takes the age, reward, history size, and last \texttt{SearchNode} type (chance or decision) of an agent and saves this information. This information is sufficient for an agent to revert itself to the time at which the \texttt{ModeUndo} was made.

}

\item{\texttt{Agent::modelRevert}

	Takes a \texttt{ModelUndo} object and reverts an agent's model to the state it was at when this \texttt{ModelUndo} was taken. It iterates through the history and rewinds objects, actions, and rewards seen, peeling them off the history array, and reverting the CTW tree to reflect this removal of information (performs the reverse of an agent's model update procedure).

}
\end{itemize}
\subsubsection{General functions}
\begin{itemize}
\item{\texttt{search}
	
	The top level function for determining an action based on an agent's state. \texttt{search} forms the search tree as it searches. Using the \texttt{SearchNode::sample} function, \texttt{search} samples from the root multiple times to form the MCTS (Monte Carlo Tree Search) tree. This function also uses \texttt{ModelUndo} to save an agent before modifying it and it's CTW tree. It can then revert it back to it's previous state before the sampling run took place.
	
	After the tree has been explored sufficiently (\texttt{numSimulations}), the best action is determined by taking the maximum expected reward over all of the possible actions. This is best action is the one that search returns.
}
	
\end{itemize}

\chapter{Experimentation}
\section{Sequence prediction}
\label{sec:Sequence prediction}
The CTW tree is the primarmy prediction mechanism in the \mac{} model. The CTW implementation of \mac{} was tested in isolation from the rest of the agent on a range of deterministic and non-deterministic sequence. This was useful for debugging purposes, and the results are an interesting biproduct of \mac{}.
\subsection{Deterministic sequence prediction}
Several simple sequences were given to the CTW tree, and the CTW tree was asked to continue the sequence. This can be seen in Figure \ref{tab:det_seq_pred}. We can see that the CTW tree is able to correctly predict the next bit, or multiple bits, in the sequence.

\begin{figure}
\bcen
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{|cc|cc|}
\hline
Sequence & Prediction & Sequence & Prediction\\
\hline $0^{1000}$ & 0... & $1^{1000}$ & 1...\\ 
\hline $(01)^{1000}$ & 01... & $(10)^{1000}$ & 10...\\ 
\hline $(0110)^{500}$ & 0110... & $(1100)^{500}$ & 1100...\\ 
\hline $(110)^{500}$ & 110... & $(001)^{500}$ & 001...\\
\hline 
\end{tabular} 
\egroup
\ecen
\caption{Some of the sequences given to the CTW tree, and the prediction of the next symbol.}
\label{tab:det_seq_pred}
\end{figure}

\subsection{Non deterministic sequence prediction}
A partially non deterministic sequence was given to CTW for testing purposes. The sequence was given to CTW was of the form\\
\[
S := x^{\text{obs}}_0,\; x^{\text{rew}}_0,\; x^{\text{act}}_1,\; x^{\text{obs}}_1,\; x^{\text{rew}}_1, x^{\text{act}}_2,\; x^{\text{obs}}_2,\; x^{\text{rew}}_2, ...
\]
Where\\
\[
x^{\text{act}}_i := r_{\text{act}}, \;\;\;\; x^{\text{obs}}_i := r_{\text{obs}}, \;\;\;\; \text{and} \;\;\;\;
x^{\text{rew}}_i :=
\begin{cases}
0 & \text{if } x^{\text{obs}}_i = x^{\text{act}}_i\\
1 & \text{if } x^{\text{obs}}_i \not= x^{\text{act}}_i
\end{cases}
\]
Where\\
\[
r_\text{act}, r_\text{obs} \text{ are random bits }
\]

The idea of this sequence $S$ is to simulate a coinflip environment history sequence. The random bits $x^{\text{obs}}_i$ and $x^{\text{act}}_i$ simulate random coin flips, and the reward bit $x^{\text{act}}_i$ compares them. The action bit $x^{\text{act}}_i$ is random in this sequence.

A sequence of this type with size ~9000 (3000 iterations) is given to the CTW tree, up to the point
\[
S = x^{\text{obs}}_0,\; x^{\text{rew}}_0,\; x^{\text{act}}_1,\;\;\;...\;\;\;,x^{\text{act}}_{3000},\;x^{\text{obs}}_{3000}
\]

The CTW tree is then asked to predict which bit is the next in the sequence. The idea of this experiment is to determine if the CTW tree ``understands'' the rules of the game - if it understood, it would predict $x^{\text{rew}}_{3000} = (x^{\text{act}}_{3000} \land x^{\text{obs}}_{3000}) \lor (\lnot x^{\text{act}}_{3000} \land \lnot x^{\text{obs}}_{3000})$.

The results of this experiment are show in Figure \ref{tab:non_det_seq_pred}. We see that it correctly predicts the reward bit given the action and observation bits. This provides some verification that the implementation of the CTW tree is correct.
\begin{figure}
\bcen
\bgroup
\def\arraystretch{1.5}
\begin{tabular}{c |c| c }
 • & $x^{\text{rew}}_{3000} = 0$ & $x^{\text{rew}}_{3000} = 1$ \\ 
\hline $x^{\text{rew}}_{3000} = 0$ & 1 & 0 \\ 
\hline $x^{\text{rew}}_{3000} = 1$ & 0 & 1 \\  
\end{tabular}
\egroup
\ecen
\caption{Predicted values for $x^{\text{rew}}_{3000}$}
\label{tab:non_det_seq_pred}
\end{figure}

\section{Coin flip results}

 Coin flip is a simple environment where the agent is given a reward of 1 for correctly guessing the next bit, where the next bit is random with a certain bias such that the next bit is 1 with probability $\theta \in [0,1]$, else the reward is 0. For a regular coin, $\theta = 0.5$, and there is no dominant strategy. However, if $\theta \not = 0.5$, dominant strategies emerge ($\theta > 0.5 \Rightarrow $ predict 1, $\; \theta < 0.5 \Rightarrow $ predict 0). The variable $\theta$ is hidden from the agent.
 
\begin{figure}[h]
\centering
 \begin{tikzpicture}
    \begin{axis}[xlabel=Cycle,ylabel=Average reward]
      \pgfplotstableread{graph_data/coinflip_p=0_2.plot.csv}\plotcsv
      \addplot+[every mark/.append style={no markers},smooth] table
      {\plotcsv};
    \end{axis}
  \end{tikzpicture}
\caption{Coinflip simulation for $\theta=0.2$}
\label{fig:coin_0_2}
\end{figure}
	
Figure \ref{fig:coin_0_2} shows the results for $\theta=0.2$. The theoretical best strategy for this environment would be to predict $0$ always, and gain a reward of $0.8$. We can see that our implementation of \mac{} acheives better than a random player (who would receive an average reward of 0.5) but does not acheive the maximum theoretical average reward in 8000 cycles.
 
% \begin{figure}[h]
%   \begin{center}
%     \begin{tikzpicture}
%       \begin{axis}[xlabel=Cycle,ylabel=Average reward,smooth,/pgfplots/no markers,legend style={
%                                                cells={anchor=east},
%                                                legend pos=outer north east,
%                                                }]
%         \foreach \p in {0, 2, 4, 6, 8} {
%         \addplot table {graph_data/coinflip_p=0_\p.plot.csv};
%         \addlegendentryexpanded{$p = 0.\p$};
%         }
%         \addplot table {graph_data/coinflip_p=1_0.plot.csv};
%         \addlegendentry{$p = 1.0$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{center}
%   \caption{Coin flip results}
%   \label{fig:coinflip_results}
% \end{figure}
Another graph with $\theta = 1$ is shown in Figure \ref{fig:coin_1_0}. With $\theta = 1$, the maximum theoretical reward is $1$, with the strategy of picking 1 always. We can see that our implementation of \mac{} quickly converges to this strategy of picking 1 always, with the average reward approaching 1.
\begin{figure}
\centering
 \begin{tikzpicture}
    \begin{axis}[xlabel=Cycle,ylabel=Average reward]
      \pgfplotstableread{graph_data/coinflip_p=1_0.plot.csv}\plotcsv
      \addplot+[every mark/.append style={no markers},smooth] table
      {\plotcsv};
    \end{axis}
  \end{tikzpicture}
\caption{Coinflip simulation for $\theta=1.0$}
\label{fig:coin_1_0}
\end{figure}

\section{Tiger results}
Tiger is a game where the agent chooses out of two doors. Each door has a 50\% chance to contain a moderate reward of 110, or contain a reward of 0. A reward of 99 is given for listening, which has an 85\% chance to correctly reveal (as an observation) the size of the reward behind the door. This is a non-deterministic environment.\\\\
\begin{center}
\begin{tabular}{| r | l | }
\hline
mc-simulations & 50\\
ct-depth & 30\\
agent-horizon & 4\\
exploration & 0.01\\
explore-decay & 0.999\\
\hline
\end{tabular}\\
\vspace{0.5mm}
Table 4.3: The parameters used in the Tiger environment.
\end{center}
The CT depth was chosen to be 30 so as to be large enough to store several percept and reward sequences. The agent horizon was set to 4 in order to enable optimum play (listen 3 times then choose the door.

 \begin{figure}[h]
   \begin{center}
     \begin{tikzpicture}
       \begin{axis}[xlabel=Cycles,ylabel=Average reward,smooth,/pgfplots/no markers]
         \addplot table {graph_data/tiger.plot.csv};
       \end{axis}
     \end{tikzpicture}
   \end{center}
   \caption{Tiger results}
   \label{fig:tiger_results}
 \end{figure}
The initial learning shows a steep curve, and the agent learns after 500 cycles that listening is gives safe reward (as a reward of 99 is better than picking a random door resulting in an average reward of 55). From this point onwards the agent seems to begin to notice the correlation between the percept provided by listen and the reward given by a door, and there is another small spike. The agent has likely had insufficient time to learn, so the reward has not converged, and is still far from the theoretical optimum of 107 average reward. This is caused by the difficulty in learning the observation reward sequence in the CTW; as the number of reward bits is 7, CTW learning takes a long time.

\section{Grid world results}
Grid world is a world where a blind agent (never given any observation) in a 4x4 grid needs to reach the bottom right corner, which will grant a reward of 1. All other rewards are 0. Moving outside the grid returns the agent back to the nearest square inside the grid. Once reaching the bottom right corner, the agent is teleported to a random other square in the grid. Note that under the specification, the agent may be teleported to the bottom right corne. The agent will not receive extra reward for this, but has an advantage in that either RIGHT or DOWN will result in an immediate reward. The environment is non-deterministic, however, contains an optimal sequence of moves that is guaranteed a reward within 6 moves, namely RIGHT, DOWN repeated 3 times, which would give an average reward of 0.33.\\\\
\begin{center}
\begin{tabular}{| r | l | }
\hline
mc-simulations & 50\\
ct-depth & 12\\
agent-horizon & 4\\
exploration & 0.01\\
explore-decay & 0.99999\\
\hline
\end{tabular}\\
\vspace{0.5mm}
Table 4.4: The parameters used in the Grid world environment.
\end{center}
These parameters are chosen such that the agent has enough depth in the CT to store the optimum strategy (depth 12), and will search a reasonable distance in the future (given the small environment).

 \begin{figure}[h]
   \begin{center}
     \begin{tikzpicture}
       \begin{axis}[xlabel=Cycles,ylabel=Average reward,smooth,/pgfplots/no markers]
         \addplot table {graph_data/grid.plot.csv};
       \end{axis}
     \end{tikzpicture}
   \end{center}
   \caption{Grid world results}
   \label{fig:grid_results}
 \end{figure}
The results for this experiment are particularly interesting because the graph shows the agent clearly wandering around lost for the first 4000 cycles, but then starting to correctly identify the better strategy, and the reward sharply increases. As shown, the agent strategy has clearly not converged yet and needs more time. The actions taken shows the agent clearly favouring sequences with RIGHT and DOWN, dramatically increasing the average reward. The learning 

\section{Biased Rock paper scissors results}
In Biased Rock Paper Scissors (RPS), the agent plays against an environment which will randomly select rock, paper or scissors; but after winning with rock, the environment will repeat a rock move. This is interesting because it creates a bias in the environment's distribution, and the agent needs to try to exploit this bias. The environment is non-deterministic. Note that optimum play on this environment would involve the agent playing scissors until a loss, then playing paper directly afterwards. \\\\
\begin{center}
\begin{tabular}{| r | l | }
\hline
mc-simulations & 50\\
ct-depth & 8\\
agent-horizon & 3\\
exploration & 0.1\\
explore-decay & 0.999\\
\hline
\end{tabular}\\
\vspace{0.5mm}
Table 4.5: The parameters used in the RPS environment.
\end{center}
The CT depth is chosen such that 2 observation and reward pairs can be stored, which will enable optimal strategy. The agent horizon is 3 for similar reasoning, slightly longer than the horizon required for optimal strategy, to ensure that the agent isn't immediately unfairly biased towards optimal strategy.
\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[xlabel=Cycles,ylabel=Average reward,smooth,/pgfplots/no markers]
        \addplot table {graph_data/rps.plot.csv};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  \caption{Biased RPS results}
  \label{fig:rps_results}
\end{figure}

The results show that the agent is initially thrown off by the curious distribution of the environment, actually performing worse than random. While this may seem odd, it must be kept in mind that the CTW will initially perform poorly for non-deterministic sequence prediction as it can be thrown off by chance. If the agent is at a point where it predicts that scissors is the best move, it is more likely to predict that scissors will be the best next move as well. The problem is that the bias of playing two rocks in a row is quite subtle and will take a while to learn the correct paths in the CTW, making it difficult to learn that a loss on scissors must be immediately followed by paper. A sharp learning curve happens in iterations 200-600 as the agent learns how to handle this distribution (by not playing scissors after losing on scissors) and the average reward becomes close to 1 (reward from performing random). However, even after 6000 iterations, the agent still hasn't managed to learn that a loss on scissors should be followed by paper for optimum reward.

\section{Kuhn Poker results}
In Kuhn Poker, the agent plays against a Nash Equilibrium AI, with gamma set to 0.5. More details about the environment can be found in Kuhn \cite{kuhn}. The agent is given a reward of 0 for a loss, 2 for a win or 1 if no showdown has occurred so far.\\\\
\begin{center}
\begin{tabular}{| r | l | }
\hline
mc-simulations & 50\\
ct-depth & 12\\
agent-horizon & 2\\
exploration & 0.01\\
explore-decay & 0.9999\\
\hline
\end{tabular}\\
\vspace{0.5mm}
Table 4.6: The parameters used in the Kuhn Poker environment.
\end{center}
The CT depth was chosen in order to be slightly more than enough to play optimally. The horizon was chosen in order to enable optimal strategy. While this could possibly be set to 3 to make sure there is no unfair bias, this is reasonable because the maximum iterations per environment cycle are 2.

\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[xlabel=Cycles,ylabel=Average reward,smooth,/pgfplots/no markers]
        \addplot table {graph_data/kuhn.plot.csv};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  \caption{Kuhn poker results}
  \label{fig:kuhn_results}
\end{figure}

Kuhn Poker initially performs poorly against the Nash player, losing significantly more often than winning. However, interestingly, within 800 cycles, the agent begins to learns how to play the basic game (such as betting when seeing a king and passing with a jack), seen by the steep improvement which cuts out just above 1.87. Some finer points are learned up to 4000 cycles, possibly such as bluffing with a queen, reaching an average reward of 1.9. However, the agent does not have enough time to start learning how to exploit the Nash player and still loses more times than it wins.

\section{Pacman results}
In partially observable Pacman, the agent plays in a maze, getting reward for consuming food but losing reward for running into walls and getting eaten by ghosts, according to the classical Pacman game. However, the agent cannot see the entire map, with a 16 bit percept based on 4 bits for line of sight for food, 3  bits for smelling food (based on manhattan distances of 2, 3 and 4), 4 bits for line of sight of ghosts, and finally 1 bit for the power pill.\\\\
Several assumptions were made on top of the specification.  While under the influence of the power pill, the agent is able to eat ghosts instead. Note that there is no reward given for this, unlike the classic pacman game. Ghosts also do not respawn, differing from traditional pacman. The effect this has is the agent should learn that if it eats a power pill, a good strategy is to try to eat ghosts, which will maximise future reward. However, given the limitations of balancing search size against computational limits, it is unlikely that this will be seen to a large degree.\\\\
The rewards were more complicated because the agent can trigger multiple events with one action and thus receive multiple rewards. A reward of -60 was given for running intot a ghost, -10 for a wall, -1 for moving, 10 for eating a pellet and 100 for finishing all pellets. Multiple events had their rewards added together, and 71 was added to the total to ensure the reward remained positive.
\begin{center}
\begin{tabular}{| r | l | }
\hline
mc-simulations & 50\\
ct-depth & 32\\
agent-horizon & 4\\
exploration & 0.01\\
explore-decay & 0.99999\\
\hline
\end{tabular}\\
\vspace{0.5mm}
Table 4.7: The parameters used in the Pacman environment.
\end{center}
Whilst the CT depth and horizon chosen as parameters for pacman are far below the sizes required to play optimally, searching the depth of an entire game would be computationally infeasible. The parameters were chosen in order to balance computational time required. The agent horizon of 4 is very small but can still enable pacman to learn some advantageous behaviours in the environment, such as not walking into the direction where a ghost can be seen in line of sight.
\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \begin{axis}[xlabel=Cycles,ylabel=Average reward,smooth,/pgfplots/no markers]
        \addplot table {graph_data/pacman.plot.csv};
      \end{axis}
    \end{tikzpicture}
  \end{center}
  \caption{Pacman results}
  \label{fig:pacman_results}
\end{figure}

The results show that the agent clearly did not have enough cycles to learn optimal behaviours. The average reward shows the agent has considerable difficulty learning not to run into walls or ghosts in the first 600 cycles. However after this it seems to begin to have enough information in the context tree to predict suboptimal reward for running into walls in one or two directions.

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}
