\documentclass[pdftex,twoside,a4paper]{report}
\usepackage[pdftex]{graphicx}
\usepackage[margin=3.0cm]{geometry}
\usepackage[english]{babel}
\usepackage[normalem]{ulem}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage[sc]{mathpazo}
\usepackage[round]{natbib}

\newcommand{\hs}{$\hspace{0.5cm}$}
\newcommand{\bt}{\begin{tabbing}}
\newcommand{\et}{\end{tabbing}}
\newcommand{\bcen}{\begin{center}}
\newcommand{\ecen}{\end{center}}
\newcommand{\mac}{MC-AIXI-CTW}

\begin{document}

\begin{titlepage}
\renewcommand*{\thefootnote}{\fnsymbol{footnote}}
\begin{center}

\textsc{\Large Research School of Computer Science}\\[0.5cm]
\textsc{\Large College of Engineering and}\\[0.2cm]
\textsc{\Large Computer Science}\\[0.5cm]
\vspace{1.4cm}
\hrule
\vspace{1.4cm}
{\huge \bfseries An Implementation of MC-AIXI-CTW} \\
\vspace{0.4cm}


\begin{tabular}{ccccc}
  Jarrah Bloomfield\footnotemark &
  Luke English\footnotemark &
  Andrew Haigh\footnotemark &
  Joshua Nelson\footnotemark &
  Anthony Voutas\footnotemark
\end{tabular}

\vspace{1.4cm}
\hrule
\vspace{1.0cm}
\textsc{\large COMP4620 - Advanced Artificial Intelligence}\\
\textsc{Assignment 2}\\
\vspace{1.0cm}
\hrule
vspace{1.4cm}
\vfill
Bottom of the page
{\large \today} \\[0.5cm]

\begin{tabular}{ccccc}
  \setcounter{footnote}{0}
  u4669875\footnotemark &
  u4667010\footnotemark &
  u4667844\footnotemark &
  u4850020\footnotemark &
  u4519169\footnotemark
\end{tabular}
\end{center}
 
\end{titlepage}
\chapter{Description of the MC-AIXI-CTW implementation}

The AIXI agent is a formal, mathematical agent which represents a solution to
the general reinforcement learning problem. Principally, AIXI consists of an
expectimax search over a Bayesian mixture of Turing machines in order to
choose optimal actions by predicting future observations and rewards based on
past experience. Given infinite computational resources, AIXI represents the
optimal reinforcement agent: maximising future expected reward in any unknown
environment.

In the far more limited world of what is tractable, we require an
approximation to AIXI. Here, we approximate AIXI via a combination of UCT
search (Monte-Carlo Tree Search with the Upper Confidence Bound)
\citep{kocsis2006bandit} and Context Tree Weighting
\citep{willems1995context}, yielding MC-AIXI-CTW.

\chapter{User Manual}
\section{Arguments}
The agent can be compile with the \texttt{make} command. The agent then can then be run using\\

\texttt{./main <environment> <logfile>}\\

\texttt{<environment>} is a compulsory argument, which specifies the environment configuration file the agent is to use. In this implementation, it is one of the following
\begin{itemize}
\item \texttt{coinflip.conf}: Biased coin flip enviroment
\item \texttt{grid.conf}: Gridworld environment
\item \texttt{kuhnpoker.conf}: Kuhn poker environment
\item \texttt{pacman.conf}: Pacman environment
\item \texttt{rps.conf}: Biased rock paper scissors environment
\item \texttt{tiger.conf}: ``Tiger'' Environment
\item \texttt{composite.conf}: A combination of the above environments
\end{itemize}
\texttt{<logfile>} is an optional argument, which specifies the name of a log file to output results to.
\newline
\section{Configuration files}
\texttt{.conf} files are \emph{configuration files}, specifying which environment is to be used, and relevant parameters for each environment. Each configuration file has the following parameters
\begin{itemize}
\item \texttt{environment}: The name of the environment to use. One of \{4x4-grid, kuhn-poker, tiger, biased-rock-paper-scissor, pacman, composite\}.
\item \texttt{exploration}: The rate at which the agent explores, by making random decisions.
\item \texttt{explore-decay }: The rate at the exploration rate decreases
\end{itemize}
In addition to this, some configurations have parameters that are specific to their environments.
\begin{itemize}
\item Coinflip
    \begin{itemize}
        \item \texttt{coin-flip-p}: The probability of a flipping heads (0 $\leq$ \texttt{coin-flip-p}  $\leq$ 1).
    \end{itemize}
\item Kuhn poker
    \begin{itemize}
        \item \texttt{gamma}: A constant that determines the environment's Nash equilibrium strategy. (0 $\leq$ \texttt{gamma}  $\leq$ 1)
    \end{itemize}
\item Pacman
    \begin{itemize}
        \item \texttt{mapfile}: The location of the map file for the pacman board.
    \end{itemize}
\item tiger.conf
    \begin{itemize}
        \item \texttt{left-door-p}: The probability that the gold is behind the left door
        \item \texttt{listen-p}: The probability that a listening observation is correct.
    \end{itemize}
\item composite.conf
    \begin{itemize}
        \item \texttt{environmentN}: Specifies the $N^{\text{th}}$ environment, where $0 \leq N \leq 10$. The value of this parameter is an integer $\leq 10$, and indicates which environment environmentN represents.
        \item \texttt{startN}: Specifies the time step that at which the $N^{\text{th}}$ environment starts, where $0 \leq N \leq 10$.
        \item Paremeters required for the environemnts $1..N$ specified in \texttt{environment.cpp}.
    \end{itemize}
\end{itemize}

\chapter{MC-AIXI-CTW Implementation}
\section{Design Choices}
\section{Context Tree Weighting}
The algorithm for implementing context-tree weighting (CTW) mainly consists of the methods and objects given in the file \texttt{predict.cpp}, along with its corresponding \texttt{.hpp} file. We run through these from the beginning of the file to its end.  

\subsection{Objects}
\begin{itemize}
\item{\texttt{CTNode}

This object represents a node in the context tree; and so it needs to store both the Laplacian estimate of the probability, along with the weight given to it by the actual CTW algorithm. For mathematical stability, we store these floating-point numbers as logarithms, so we can just add them instead of multiplying them. In addition, it has a 2-array of pointers to its left and right child, and it stores the counts for these nodes in the context of the history (which is also required for the CTW algorithm).
  }
\item{\texttt{ContextTree}

Here we have the entire context tree represented as an object, which is a glorified pointer to the root node of the tree (as \texttt{CTNode}s store their own children). In addition to storing the root, \texttt{ContextTree} also stores a double-ended queue of symbols (boolean integers) which represents the current history of the tree, and an unsigned integer which represents the maximum depth of the tree (so that we don't add nodes past the depth, and take up more memory than we want).
  }
\end{itemize}

\subsection{Methods}
\subsubsection{\texttt{CTNode}}
\begin{itemize}
\item{\texttt{logProbWeighted, logProbEstimated, visits, child}

    These methods are simply defined. Since the values stored in variables such as \texttt{m_log_prob_weighted} are declared privately, it is required to use a public method to get them. \texttt{visits} grabs the sum of the counts for the child nodes - the number of times that we have seen this particular node in our travels; and \texttt{child} takes a symbol, and returns a pointer to the correct (left or right) child.
  }
\item{\texttt{update}

    \texttt{update} is one of the most important methods in the entire CTW implementation. We recurse through the tree, starting from the node that calls this method, popping symbols from the end of the history until we reach a leaf; as in the CTW algorithm. When we do, we update the counts for the leaf based on the symbol that is passed to \texttt{update}, and then follow the branch back up the tree, using the equation given for $P_{w}$: \[ P_{w} = \frac{1}{2}P_{KT}(n) + \frac{1}{2}P^{0}P^{1}. \] % can someone check this against the literature?
This method doesn't 
  }
\item{\texttt{size}
    
  }
\item{\texttt{logKTMul}
    
  }
\item{\texttt{revert}
    
  }
\item{\texttt{prettyPrintNode}
    
  }
\end{itemize}
\subsubsection{\texttt{ContextTree}}
\begin{itemize}
\item{\texttt{clear}
    
  }
\item{\texttt{update}

    This is an overloaded method, which can either take in a symbol, or a list of symbols. If passed a list, it simply runs itself on each of the symbols in turn; and if passed a symbol, it first checks whether we have enough pre-history to generate a tree. If we don't, it simply pushes the symbol to the back of the history (where we can easily access it again), and if we do, then we need to actually change nodes; so we call the \texttt{update} method from \texttt{CTNode} instead, before pushing the symbol onto the history.
  }
\item{\texttt{updateHistory}
    
  }
\item{\texttt{revert}
    
  }
\item{\texttt{revertHistory}
    
  }
\item{\texttt{predict}

  }
\item{\texttt{genRandomSymbols}
    
  }
\item{\texttt{genRandomSymbolsAndUpdate}
    
  }
\item{\texttt{logBlockProbability}
    
  }
\item{\texttt{nthHistorySymbol}
    
  }
\item{\texttt{depth, historySize, size}
    
  }
\item{\texttt{predictNext}
    
  }
\item{\texttt{prettyPrint, prettyPrintNode, printHistory}
    
  }
\end{itemize}
\section{Upper Confidence Tree}

\section{Revert Function}

\chapter{Experimentation}
\label{sec:Experimentation}

% This is just a test figure, to see what happens with PGFPlots and such.
\begin{figure}[h]
\begin{center}
  \begin{tikzpicture}
    \begin{axis}[xlabel=Cycle,ylabel=Average reward,title={Coinflip $p=0.0$}]
      \pgfplotstableread{plot.csv}\plotcsv
      \addplot+[every mark/.append style={no markers},smooth] table
      {\plotcsv};
    \end{axis}
  \end{tikzpicture}
\end{center}
\end{figure}

\section{Learning}
\label{sec:Learning}
\subsection{Sequence prediction}
The CTW tree is the primarmy prediction mechanism in the \mac{} model. The CTW implementation of \mac{} was tested in isolation from the rest of the agent on a range of deterministic and non-deterministic sequence.

\section{Extension}
\label{sec:Extension}

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}